import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
import joblib
import os
import glob
from data_loader import DataLoader
from spam_protector import SpamProtector

class ModelManager:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1500, stop_words=['–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∑–∞', '–∫'])
        self.group_encoder = LabelEncoder()
        self.expert_encoder = LabelEncoder()
        self.label_encoder = LabelEncoder()
        
        self.group_classifier = None
        self.expert_classifier = None
        self.label_classifier = None
        
        self.data_loader = DataLoader()
        self.spam_protector = SpamProtector()
        self.is_trained = False
        self.confidence_threshold = 0.25  # –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ 25%
        
    def load_and_train(self, folder_path="–í—ã–≥—Ä—É–∑–∫–∞"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        success = self.data_loader.load_from_excel(folder_path)
        if not success:
            return False
        return self._train_model()
    
    def _train_model(self):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if len(self.data_loader.historical_data) < 10:
            print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return False
            
        try:
            df = pd.DataFrame(self.data_loader.historical_data)
            
            # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            X = self.vectorizer.fit_transform(df['full_text'])
            
            # –û–±—É—á–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –≥—Ä—É–ø–ø
            groups_encoded = self.group_encoder.fit_transform(df['group'])
            self.group_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
            self.group_classifier.fit(X, groups_encoded)
            
            # –û–±—É—á–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
            experts_encoded = self.expert_encoder.fit_transform(df['expert'])
            self.expert_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
            self.expert_classifier.fit(X, experts_encoded)
            
            # –û–±—É—á–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –º–µ—Ç–æ–∫
            labels_encoded = self.label_encoder.fit_transform(df['label'])
            self.label_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
            self.label_classifier.fit(X, labels_encoded)
            
            self.is_trained = True
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ {len(df)} –∑–∞—è–≤–∫–∞—Ö")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def predict(self, title, description):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã, —ç–∫—Å–ø–µ—Ä—Ç–∞ –∏ –º–µ—Ç–∫–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ —Å–ø–∞–º –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"""
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º –ø–µ—Ä–µ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º
        is_spam, spam_message = self.spam_protector.is_spam(title, description)
        if is_spam:
            return {
                "group": "–°–ü–ê–ú-–§–ò–õ–¨–¢–†",
                "expert": "–°–∏—Å—Ç–µ–º–∞ –∑–∞—â–∏—Ç—ã",
                "label": "–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ",
                "confidence": 0.0,
                "group_confidence": 0.0,
                "expert_confidence": 0.0,
                "label_confidence": 0.0,
                "is_spam": True,
                "spam_message": spam_message,
                "message": "–ó–∞–ø—Ä–æ—Å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Å–ø–∞–º-—Ñ–∏–ª—å—Ç—Ä–æ–º",
                "needs_moderation": True,
                "moderation_reason": "–û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–ø–∞–º"
            }
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞, –æ–±—É—á–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å
        if not self.is_trained:
            return self._fallback_prediction(title, description)
            
        try:
            full_text = f"{title}. {description}" if description else title
            X = self.vectorizer.transform([full_text])
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≥—Ä—É–ø–ø—É
            group_encoded = self.group_classifier.predict(X)[0]
            group = self.group_encoder.inverse_transform([group_encoded])[0]
            group_confidence = np.max(self.group_classifier.predict_proba(X))
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —ç–∫—Å–ø–µ—Ä—Ç–∞
            expert_encoded = self.expert_classifier.predict(X)[0]
            expert = self.expert_encoder.inverse_transform([expert_encoded])[0]
            expert_confidence = np.max(self.expert_classifier.predict_proba(X))
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç–∫—É
            label_encoded = self.label_classifier.predict(X)[0]
            label = self.label_encoder.inverse_transform([label_encoded])[0]
            label_confidence = np.max(self.label_classifier.predict_proba(X))
            
            confidence = min(group_confidence, expert_confidence, label_confidence)
            
            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
            needs_moderation = False
            moderation_reason = ""
            
            if confidence < self.confidence_threshold:
                needs_moderation = True
                if confidence < 0.1:
                    moderation_reason = "–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: –≤–æ–∑–º–æ–∂–Ω–∞—è –æ–ø–µ—á–∞—Ç–∫–∞ –∏–ª–∏ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å"
                else:
                    moderation_reason = f"–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ ({confidence:.1%}) - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ–ª–æ–≤–µ–∫–æ–º"
            
            # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –Ω–∞ –Ω–∏–∑–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            low_confidence_components = []
            if group_confidence < self.confidence_threshold:
                low_confidence_components.append("–≥—Ä—É–ø–ø–∞")
            if expert_confidence < self.confidence_threshold:
                low_confidence_components.append("—ç–∫—Å–ø–µ—Ä—Ç")
            if label_confidence < self.confidence_threshold:
                low_confidence_components.append("–º–µ—Ç–∫–∞")
            
            if low_confidence_components and not needs_moderation:
                needs_moderation = True
                moderation_reason = f"–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏: {', '.join(low_confidence_components)}"
            
            result = {
                "group": group,
                "expert": expert,
                "label": label,
                "confidence": round(confidence, 3),
                "group_confidence": round(group_confidence, 3),
                "expert_confidence": round(expert_confidence, 3),
                "label_confidence": round(label_confidence, 3),
                "is_spam": False,
                "needs_moderation": needs_moderation,
                "moderation_reason": moderation_reason
            }
            
            return result
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return self._fallback_prediction(title, description)
    
    def _fallback_prediction(self, title, description):
        """–†–µ–∑–µ—Ä–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞"""
        return {
            "group": "–û–±—â–∞—è –≥—Ä—É–ø–ø–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏",
            "expert": "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–µ—Ä–≤–æ–π –ª–∏–Ω–∏–∏",
            "label": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∑–∞—è–≤–∫–∞",
            "confidence": 0.1,
            "group_confidence": 0.1,
            "expert_confidence": 0.1,
            "label_confidence": 0.1,
            "fallback": True,
            "is_spam": False,
            "needs_moderation": True,
            "moderation_reason": "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞ - —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞",
            "message": "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ /load_excel"
        }
    
    def set_confidence_threshold(self, threshold):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        if 0 <= threshold <= 1:
            self.confidence_threshold = threshold
            print(f"‚úÖ –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {threshold:.1%}")
            return True
        else:
            print("‚ùå –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–µ–∂–¥—É 0 –∏ 1")
            return False
    
    def save_model(self, folder_path="model"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            os.makedirs(folder_path, exist_ok=True)
            
            joblib.dump(self.vectorizer, os.path.join(folder_path, "vectorizer.joblib"))
            joblib.dump(self.group_encoder, os.path.join(folder_path, "group_encoder.joblib"))
            joblib.dump(self.expert_encoder, os.path.join(folder_path, "expert_encoder.joblib"))
            joblib.dump(self.label_encoder, os.path.join(folder_path, "label_encoder.joblib"))
            joblib.dump(self.group_classifier, os.path.join(folder_path, "group_classifier.joblib"))
            joblib.dump(self.expert_classifier, os.path.join(folder_path, "expert_classifier.joblib"))
            joblib.dump(self.label_classifier, os.path.join(folder_path, "label_classifier.joblib"))
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            config = {
                'confidence_threshold': self.confidence_threshold
            }
            joblib.dump(config, os.path.join(folder_path, "config.joblib"))
            
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫—É {folder_path}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def load_model(self, folder_path="model"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            self.vectorizer = joblib.load(os.path.join(folder_path, "vectorizer.joblib"))
            self.group_encoder = joblib.load(os.path.join(folder_path, "group_encoder.joblib"))
            self.expert_encoder = joblib.load(os.path.join(folder_path, "expert_encoder.joblib"))
            self.label_encoder = joblib.load(os.path.join(folder_path, "label_encoder.joblib"))
            self.group_classifier = joblib.load(os.path.join(folder_path, "group_classifier.joblib"))
            self.expert_classifier = joblib.load(os.path.join(folder_path, "expert_classifier.joblib"))
            self.label_classifier = joblib.load(os.path.join(folder_path, "label_classifier.joblib"))
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            try:
                config = joblib.load(os.path.join(folder_path, "config.joblib"))
                self.confidence_threshold = config.get('confidence_threshold', 0.25)
            except:
                self.confidence_threshold = 0.25
            
            self.is_trained = True
            print(f"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ø–∞–ø–∫–∏ {folder_path}")
            print(f"üìä –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {self.confidence_threshold:.1%}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
    def clear_model(self, folder_path="model"):
        """–û—á–∏—Å—Ç–∫–∞ –º–æ–¥–µ–ª–∏ - —É–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            if not os.path.exists(folder_path):
                print(f"üì≠ –ü–∞–ø–∫–∞ {folder_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                return True
                
            files = glob.glob(os.path.join(folder_path, "*"))
            for file in files:
                try:
                    os.remove(file)
                    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Ñ–∞–π–ª: {os.path.basename(file)}")
                except Exception as e:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª {file}: {e}")
            
            self.vectorizer = TfidfVectorizer(max_features=1500, stop_words=['–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∑–∞', '–∫'])
            self.group_encoder = LabelEncoder()
            self.expert_encoder = LabelEncoder()
            self.label_encoder = LabelEncoder()
            
            self.group_classifier = None
            self.expert_classifier = None
            self.label_classifier = None
            
            self.data_loader.historical_data = []
            self.data_loader.groups = set()
            self.data_loader.experts = set()
            self.data_loader.labels = set()
            
            self.is_trained = False
            self.confidence_threshold = 0.25
            
            print(f"üßπ –ú–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω–∞. –£–¥–∞–ª–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤.")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def get_data_stats(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞–Ω–Ω—ã—Ö"""
        return self.data_loader.get_stats()
    
    def get_groups(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≥—Ä—É–ø–ø"""
        return self.data_loader.get_groups()
    
    def get_experts(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤"""
        return self.data_loader.get_experts()
    
    def get_labels(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –º–µ—Ç–æ–∫"""
        return self.data_loader.get_labels()
    
    def get_model_info(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
        if not self.is_trained:
            return {
                "is_trained": False,
                "message": "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞",
                "confidence_threshold": self.confidence_threshold
            }
        
        return {
            "is_trained": True,
            "groups_count": len(self.group_encoder.classes_),
            "experts_count": len(self.expert_encoder.classes_),
            "labels_count": len(self.label_encoder.classes_),
            "groups": self.group_encoder.classes_.tolist(),
            "experts": self.expert_encoder.classes_.tolist(),
            "labels": self.label_encoder.classes_.tolist(),
            "confidence_threshold": self.confidence_threshold
        }